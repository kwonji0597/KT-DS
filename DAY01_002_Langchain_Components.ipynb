{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LangChain의 개념과 주요 컴포넌트 이해\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain이란 \n",
    "\n",
    "핵심 내용:\n",
    "- **LangChain**은 LLM 기반 애플리케이션 개발을 위한 프레임워크\n",
    "\n",
    "- **Chain**은 작업을 순차적으로 실행하는 파이프라인 구조를 제공\n",
    "\n",
    "- **Agent**는 자율적 의사결정이 가능한 실행 단위\n",
    "\n",
    "결론:\n",
    "- LangChain은 Chain과 Agent라는 두 가지 핵심 기능을 통해 LLM 애플리케이션 개발을 효율적으로 지원\n",
    "\n",
    "\n",
    "    <div style=\"text-align: center;\">\n",
    "        <img src=\"https://python.langchain.com/svg/langchain_stack_112024_dark.svg\" \n",
    "            alt=\"langchain_stack\" \n",
    "            width=\"600\" \n",
    "            style=\"border: 0;\">\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain 컴포넌트 \n",
    "\n",
    "핵심 내용:\n",
    "- LangChain **주요 컴포넌트**: LLM/ChatModel, Prompt, Memory, Tool, Document Loader, Text Splitter, Embedding, Vectorstore\n",
    "\n",
    "- **언어 처리 기능**은 LLM/ChatModel이 중심이 되며, Prompt와 Memory로 대화를 관리\n",
    "\n",
    "- **문서 처리와 검색**은 Document Loader, Text Splitter, Embedding, Vectorstore가 담당\n",
    "\n",
    "- **모듈성**이 핵심 특징으로, 독립적인 컴포넌트들을 조합해 RAG와 같은 복잡한 시스템을 구현 가능 \n",
    "\n",
    "결론:\n",
    "- Tool을 통한 확장성과 컴포넌트의 재사용성으로 다양한 LLM 애플리케이션 개발이 가능\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경 설정 및 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환경 변수 로드\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 모델 (Models)\n",
    "- LLM, ChatModel 등으로 구분\n",
    "- OpenAI, Anthropic, Google 등 다양한 모델을 지원\n",
    "- 텍스트 생성, 대화, 요약 등의 작업을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI # langchain의 ChatOpenAI함수 사용\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\")        #모델 사용하겠다는 정의\n",
    "response = model.invoke(\"안녕하세요!\")          # 이 모델에 프롬프트어 invoke 후 response로 응답 하는 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 어떻게 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 10, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_79b79be41f', 'id': 'chatcmpl-Bc6Cev6PvY7b6ySaOQu2VdfNUxDTL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f394baf9-dae3-4f90-9838-1552696c084c-0', usage_metadata={'input_tokens': 10, 'output_tokens': 10, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 응답 객체 \n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변:  안녕하세요! 어떻게 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "print(\"답변: \", response.content) # 답변 내용  프린트(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메타데이터:  {'token_usage': {'completion_tokens': 10, 'prompt_tokens': 10, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_79b79be41f', 'id': 'chatcmpl-Bc6Cev6PvY7b6ySaOQu2VdfNUxDTL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"메타데이터: \", response.response_metadata) # 답변 내용 메타 데이터(response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 메시지 (Messages)\n",
    "- Chat Model에서 사용할 수 있는 통합된 메시지 형식을 제공\n",
    "- 각 모델 제공자의 특정 메시지 형식을 신경 쓰지 않고도 다양한 채팅 모델을 활용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1. HumanMessage`\n",
    "- 사용자 역할에 해당\n",
    "- 사용자의 입력을 처리 -  Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변:  \"Glory\"는 한국어로 보통 \"영광\" 또는 \"영예\"라고 번역됩니다. 문맥에 따라 \"영광\", \"명예\", \"영예\" 등으로 사용할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage # HumanMessage 함수 사용하겠다\n",
    "\n",
    "# 사용자 메시지 생성\n",
    "human_message = HumanMessage(content=\"Glory를 한국어로 번역해주세요.\") # HumanMessage함수로 내용 입력(content)  - 사용자의 입력을 처리 -  Human\n",
    "\n",
    "# 번역 요청\n",
    "response = model.invoke([human_message])    # human_message에 입력된 내용 model을 사용하여 응답결과\n",
    "\n",
    "# 답변 출력\n",
    "print(\"답변: \", response.content)           # response의 내용 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"Glory\"의 한국어 번역은 상황에 따라 다르지만, 일반적으로 다음과 같이 번역할 수 있습니다:\\n\\n- 영광\\n- 영예\\n- 명예\\n\\n문맥에 맞게 적절한 단어를 선택하시면 됩니다. 추가적인 문장이 있다면 더 정확한 번역을 도와드릴 수 있습니다!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 17, 'total_tokens': 92, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_79b79be41f', 'id': 'chatcmpl-BbJbBjEG44x7iGqs3m31p8Q36q9Kn', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--17566458-159e-4c50-8952-98462b2e6f10-0', usage_metadata={'input_tokens': 17, 'output_tokens': 75, 'total_tokens': 92, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문자열을 입력하면, 자동으로 HumanMessage로 변환하여 요청\n",
    "model.invoke(\"Glory를 한국어로 번역해주세요.\")  # 이렇게 하면 응답결과의 메타데이터 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2. AIMessage`\n",
    "- AI 모델의 응답을 표현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"Glory\"는 한국어로 보통 \"영광\" 또는 \"영예\"라고 번역됩니다. 문맥에 따라 \"영광\", \"명예\", \"영예\" 등으로 사용할 수 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 17, 'total_tokens': 63, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_79b79be41f', 'id': 'chatcmpl-BbJXI2BctXo1AbqLzw8OheWghkmmW', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--288516cb-0b0e-431c-9eff-2dee58567298-0', usage_metadata={'input_tokens': 17, 'output_tokens': 46, 'total_tokens': 63, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AI 모델의 응답 객체를 출력 \n",
    "response    # 위에서 동일하게 응답결과 메타데이터 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Glory\"는 한국어로 보통 \"영광\" 또는 \"영예\"라고 번역됩니다. 문맥에 따라 \"영광\", \"명예\", \"영예\" 등으로 사용할 수 있습니다.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 응답 텍스트 부분을 출력\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 17,\n",
       " 'output_tokens': 46,\n",
       " 'total_tokens': 63,\n",
       " 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       " 'output_token_details': {'audio': 0, 'reasoning': 0}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 사용량 출력\n",
    "response.usage_metadata # usage_metadata (토큰 사용량)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3. SystemMessage`\n",
    "- 시스템 역할에 해당\n",
    "- AI 모델의 동작과 제약사항을 정의하는데 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessage(content='당신은 영어를 한국어로 번역하는 AI 어시스턴트입니다.', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage  # SystemMessage 함수사용(시스템 역할을 부여함) -  - AI 모델의 동작과 제약사항을 정의하는데 사용\n",
    "\n",
    "# 시스템 메시지 생성\n",
    "system_msg = SystemMessage(content=\"당신은 영어를 한국어로 번역하는 AI 어시스턴트입니다.\")  # 시스템메세지에 역할 부여\n",
    "\n",
    "system_msg  # 이렇게 하면 system_msg 의 메타데이터 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변:  영광\n"
     ]
    }
   ],
   "source": [
    "# 번역 요청\n",
    "human_message = HumanMessage(content=\"Glory\") # 휴먼메세지 내용(content)에 \"Glory\"넣고\n",
    "\n",
    "messages = [system_msg, human_message]      # messages = 시스템 메세지(system_msg) + 휴먼메세지(human_message)\n",
    "\n",
    "response = model.invoke(messages)           # 모델에 messages 인보크\n",
    "\n",
    "# 답변 출력\n",
    "print(\"답변: \", response.content)           # response의 내용 출력력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 프롬프트 템플릿 (Prompt Template)\n",
    "- 프롬프트 템플릿을 통해 일관된 입력 형식을 제공\n",
    "    1. 사용자의 입력과 파라미터를 언어 모델이 이해할 수 있는 형태로 변환하는 도구\n",
    "    2. 언어 모델에게 전달할 지시문을 만드는 틀\n",
    "- 변수를 포함한 동적 프롬프트 생성이 가능\n",
    "    1. 모든 템플릿은 딕셔너리 형태의 입력을 받아서 처리\n",
    "    2. 출력은 PromptValue 형태로 반환되며, 이는 문자열이나 메시지 리스트로 변환 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1. 문자열 프롬프트 템플릿 (String PromptTemplate)`\n",
    "- 가장 기본적인 형태\n",
    "- 단일 문자열을 형식화하는데 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='고양이에 대한 이야기를 해줘')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate   # 문자열 프롬프트 템플릿(언어 모델에게 전달할 지시문을 만드는 틀), 출력은 PromptValue 형태로 반환\n",
    "\n",
    "# 템플릿 생성 - 생성하는 구조 : PromptTemplate.from_template(\"내용입력\")\n",
    "template = PromptTemplate.from_template(\"{topic}에 대한 이야기를 해줘\") # PromptTemplate 함수에 \"{topic}변수 + 에 대한 이야기를 해줘\" 문자열을 template에 넣는다.\n",
    "# 템플릿 사용\n",
    "prompt = template.invoke({\"topic\": \"고양이\"})       # 템플릿에 {topic} 변수 \"고양이\" invoke\n",
    "\n",
    "# 출력\n",
    "prompt          # 출력 결과 : StringPromptValue(text='고양이에 대한 이야기를 해줘') - > 이렇게 하면 메타데이터 저장됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2. 채팅 프롬프트 템플릿 (ChatPromptTemplate)`\n",
    "- 여러 메시지를 포함하는 대화형 템플릿을 만들 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='당신은 도움이 되는 비서입니다', additional_kwargs={}, response_metadata={}), HumanMessage(content='인공지능에 대해 설명해주세요', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate   # 여러 메시지를 포함하는 대화형 템플릿을 만들 때 사용\n",
    "\n",
    "# 채팅 템플릿 생성 : ChatPromptTemplate함수에 system, user 각각 template에 넣는다\n",
    "# 템플릿 생성 - 생성하는 구조 : ChatPromptTemplate.from_message (system, user  메세지를 각각 넣는다 )\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "\n",
    "    (\"system\", \"당신은 도움이 되는 비서입니다\"),    # 시스템 메세지 , \"system\"대신에 \"ai\" 써도 됨\n",
    "    (\"user\", \"{subject}에 대해 설명해주세요\")       # 휴먼 메세지 - user(사용자 메세지)에  \"변수 {subject}변수 + 에 대해 설명해주세요\"\n",
    "])\n",
    "\n",
    "# 템플릿 사용\n",
    "prompt = template.invoke({\"subject\": \"인공지능\"})   # {subject}변수에 \"인공지능\" 넣고 인보크\n",
    "\n",
    "# 출력\n",
    "prompt          # template의 메타데이터 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3. 메시지 플레이스홀더 (MessagesPlaceholder)`\n",
    "- 기존 메시지 목록을 템플릿의 특정 위치에 삽입할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='당신은 도움이 되는 비서입니다', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='안녕하세요!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder  # ChatPromptTemplate, MessagesPlaceholder 함수 사용\n",
    "from langchain_core.messages import HumanMessage                            # HumanMessage 함수 사용\n",
    "\n",
    "# 메시지 플레이스홀더가 있는 템플릿 : ChatPromptTemplate함수에 system, MessagesPlaceholder 각각 template에 넣는다\n",
    "# MessagesPlaceholder - 기존 메시지 목록을 템플릿의 특정 위치에 삽입할 때 사용\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 도움이 되는 비서입니다\"),            # 역할 부여 (system 메세지) : \" 당신은 도움이 되는 비서입니다.\"\n",
    "    MessagesPlaceholder(\"chat_history\")                 # system메세지 뒤에 chat_history 의 메세지를 붙여넣음\n",
    "])\n",
    "\n",
    "# 템플릿 사용\n",
    "prompt = template.invoke({\n",
    "    \"chat_history\": [HumanMessage(content=\"안녕하세요!\")]   # chat_history 메세지 정의 : HumanMessage메세지 : 안녕하세요\n",
    "})\n",
    "\n",
    "# 결국 아래와 동일한 구조를 MessagesPlaceholder를 사용해서 특정 메세지에다가 붙여넣을 수 있도록 함.\n",
    "# template = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"당신은 도움이 되는 비서입니다\"),            # 역할 부여 (system 메세지) : \" 당신은 도움이 되는 비서입니다.\"\n",
    "#     (\"user\", \"안녕하세요\")\n",
    "# ])\n",
    "\n",
    "# 출력\n",
    "prompt.messages # messages메타데이터 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 출력 파서 (Output Parser)\n",
    "1. **역할과 기능**\n",
    "    - 모델의 텍스트 출력을 구조화된 데이터로 변환\n",
    "    - 채팅 모델과 LLM의 출력을 정규화\n",
    "    - 다운스트림 작업을 위한 데이터 형식 변환\n",
    "\n",
    "2. **사용 시 고려사항**\n",
    "    - OpenAI function calling과 같은 기능이 있는 경우, 해당 기능을 우선 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) StrOutputParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울은 대한민국의 수도이자 최대 도시로서 다양한 특징을 가지고 있습니다. 주요 특징을 몇 가지 소개해드리겠습니다.\n",
      "\n",
      "1. **정치·행정 중심지**  \n",
      "서울은 대한민국의 정부청사, 대통령 집무실(청와대), 국회의사당 등이 위치해 있어 국가의 정치·행정 중심지 역할을 합니다.\n",
      "\n",
      "2. **경제의 중심지**  \n",
      "국내의 주요 기업 본사와 금융기관들이 밀집해 있으며, 아시아를 대표하는 경제도시로 자리 잡고 있습니다. 대한민국 증권거래소(코스피)도 서울에 있습니다.\n",
      "\n",
      "3. **문화와 역사**  \n",
      "서울에는 경복궁, 창덕궁 등 조선시대의 궁궐과 고궁이 잘 보존되어 있으며, 전통과 현대가 공존하는 도시입니다. 또한 다양한 박물관, 미술관, 공연장이 있어 문화 생활이 풍부합니다.\n",
      "\n",
      "4. **교통의 요지**  \n",
      "지하철, 버스, 고속도로 등 발달된 교통망을 갖추고 있어 국내외 이동이 편리합니다. 인천국제공항과 김포공항을 통해 국제 항공망도 잘 연결되어 있습니다.\n",
      "\n",
      "5. **교육과 연구의 중심**  \n",
      "서울에는 서울대학교, 연세대학교, 고려대학교 등 명문 대학들이 위치해 있으며, 다양한 연구기관과 스타트업이 모여 교육과 연구 혁신의 거점입니다.\n",
      "\n",
      "6. **다양한 생활 인프라와 편의시설**  \n",
      "대형 백화점, 쇼핑몰, 음식점, 카페 등 다양한 편의시설과 생활 인프라가 잘 갖추어져 있어 삶의 질이 높습니다.\n",
      "\n",
      "7. **인구와 도시 규모**  \n",
      "약 천만 명에 달하는 인구가 거주하는 대도시로서, 높은 인구 밀집도와 다문화 사회의 특징도 가지고 있습니다.\n",
      "\n",
      "이 외에도 서울은 첨단 기술과 스마트 시티 정책을 추진하며 지속 가능한 도시 발전에 힘쓰고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser   # StrOutputParser 함수 사용 - 모델의 텍스트 출력을 구조화된 데이터로 변환\n",
    "from langchain_core.prompts import PromptTemplate           # PromptTemplate 함수 사용 -  # 문자열 프롬프트 템플릿(언어 모델에게 전달할 지시문을 만드는 틀), 출력은 PromptValue 형태로 반환\n",
    "from langchain_openai import ChatOpenAI                     # ChatOpenAI 함수 사용 \n",
    "\n",
    "# 기본적인 문자열 파서 사용\n",
    "# StrOutputParser : 텍스트 출력을 구조화 데이터로 변환.. json으로 주로 출력할때 사용한다.\n",
    "parser = StrOutputParser() #실제적으로 텍스트 필요할때 사용한다.  이거 사용안하면 [SystemMessage(content= 이런식으로 표현됨\n",
    "\n",
    "# 프롬프트 템플릿 설정 - PromptTemplate : 언어 모델에게 전달할 지시문을 만드는 틀. \n",
    "# PromptTemplate 일반적 틀 : PromptTemplate.from_template(\"내용입력\")\n",
    "prompt = PromptTemplate.from_template(\"도시 {city}의 특징을 알려주세요\")    # PromptTemplate 함수에 \"도시 {city}변수 + 의 특징을 알려주세요\" 문자열을 prompt(프롬프트 템플릿)에 넣는다.\n",
    "\n",
    "# 모델 정의\n",
    "model = ChatOpenAI(model='gpt-4.1-mini')\n",
    "\n",
    "# 체인 구성\n",
    "chain = prompt | model | parser #입력/ 모델 / 출력\n",
    "\n",
    "# 체인 실행 (prompt입력 + model엔진 모델 + parser출력을 하나로 묶어준다는 의미..)\n",
    "result = chain.invoke({\"city\": \"서울\"}) # 프롬프트가 이 값을 기다림\n",
    "\n",
    "# 결과 출력\n",
    "print(result)   # result가 메타데이터로 안나오고, parser와 묶었기 때문에(StrOutputParser()) 데이터로 구조화 되어서 나옴 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 구조화된 출력 (with_structured_output 메소드)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field # pydantic 라이브러리 이용해서 클래스형태로 스키마 이용하는 형태가 요즘 사용\n",
    "\n",
    "# Pydantic 클래스로 출력 구조를 정의\n",
    "class CityInfo(BaseModel):\n",
    "    name: str = Field(description=\"도시 이름\")\n",
    "    description: str = Field(description=\"도시의 특징\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "도시 이름: 서울\n",
      "특징: 서울은 대한민국의 수도로, 역사와 현대가 어우러진 대도시입니다. 한강을 중심으로 발전하였으며, 다양한 문화유산과 첨단 기술 산업이 공존하는 곳입니다. 또한, 쇼핑, 음식, 예술 등 다양한 문화 활동이 활발하게 이루어지고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field           # BaseModel : 데이터 구조를 정의하고, 자동으로 타입 검사 및 유효성 검사를 해주는 클래스\n",
    "                                                # Field : 각 필드(속성)에 대해 추가 설정을 지정\n",
    "                                                # Field()는 기본값, 설명, 유효성 조건(예: 최소값, 최대값), 예시(example) 등을 지정\n",
    "                                                \n",
    "from langchain_core.prompts import PromptTemplate   # PromptTemplate : 언어 모델에게 전달할 지시문을 만드는 틀. \n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1. 출력 스키마 정의 - 아래 구조로 출력을 정규화 하기 위해 클래스 정의\n",
    "class CityInfo(BaseModel):      # BaseModel함수를 전체 다 인자로 받는 CityInfo 클래스 정의 (데이터 구조 정의, 유효성 검사)\n",
    "    name: str = Field(description=\"도시 이름\")              # str의 필드를 생성하고, \"도시 이름\"으로 필드를 설명한다. # description : 필드설명 \n",
    "    description: str = Field(description=\"도시의 특징\")     # str의 필드 설명은 \"도시의 특징\"\n",
    "\n",
    "\n",
    "# 2. 프롬프트 템플릿 생성 - PromptTemplate.from_template(\"내용입력\") - 언어모델에게 전달할 지시문을 만드는 것\n",
    "prompt = PromptTemplate.from_template(\"도시 {city}의 특징을 알려주세요.\") # 도시이름을 외부에서 전달 받을거임\n",
    "\n",
    "# 3. 모델 생성 및 구조화된 출력 바인딩\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)     # 모델 정의\n",
    "structured_model = model.with_structured_output(CityInfo)   # 클래스로 만든 출력 스키마를 모델에게 전달해서, 출력을 구조화 함\n",
    "\n",
    "# 4. 프롬프트와 모델 체인 연결\n",
    "chain = prompt | structured_model # 프롬프트 지시문 과 구조화된 출력 모델을 연결 -> 그러면 클래스로 만든 스키마로 출력됨\n",
    "\n",
    "# 5. 체인 실행\n",
    "result = chain.invoke({\"city\": \"서울\"})\n",
    "\n",
    "# 6. 결과 출력 (CityInfo 객체)\n",
    "print(f\"도시 이름: {result.name}\")\n",
    "print(f\"특징: {result.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 메모리 (Memory)\n",
    "- 대화 기록을 저장하고 관리\n",
    "- 컨텍스트 유지를 위한 다양한 메모리 타입을 제공\n",
    "- 대화 요약, 버퍼링 등의 기능을 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain 라이브러리에서 사용하는 추상 클래스.. 채팅 메시지 히스토리(대화 이력)를 저장하거나 불러오기 위한 기본 인터페이스 역할\n",
    "from langchain_core.chat_history import BaseChatMessageHistory \n",
    "\n",
    "# **대화 기록(chat history)**이나 **메모리(memory)**를 사용할 때\n",
    "# 하위 클래스 : HumanMessage(사용자가 보낸 메시지), AIMessage(AI가 응답한 메세지), SystemMessage(시스템에서 주는 메시지 (지침 등)), \n",
    "from langchain_core.messages import BaseMessage # 채팅 메시지의 추상(base) 클래스. **대화 기록(chat history)**이나 **메모리(memory)**를 사용할 때\n",
    "\n",
    "from pydantic import BaseModel, Field       # # BaseModel : 데이터 구조를 정의하고,\n",
    "from typing import List                     # 어떤 변수나 함수의 매개변수가 리스트(List) 형식임을 명확하게 알려줌.\n",
    "\n",
    "# 메모리 기반 히스토리 구현\n",
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):   # 구조화된 데이터를 -채팅 히스토리 메세지에  InMemory... 메모리 기반 히스토리 구현..\n",
    "    \n",
    "    # messages: 채팅 메시지를 담는 리스트입니다. 메시지 타입은 BaseMessage 기반 (HumanMessage, AIMessage 등).\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)   # default_factory=list: 기본값을 빈 리스트로 설정 → 초기화 시 자동으로 빈 리스트 생성됨.\n",
    "    \n",
    "    # 이 메서드는 여러 개의 메시지를 한꺼번에 추가할 수 있게 해줍니다..\n",
    "    def add_messages(self, messages: List[BaseMessage]) -> None:    # add_messages 로 메세지 정의 \n",
    "        self.messages.extend(messages)\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        self.messages = []\n",
    "\n",
    "# 세션 저장소\n",
    "store = {}\n",
    "\n",
    "# 세션 ID로 히스토리 가져오기\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory: # 세션아이디를 st\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅 모델과 프롬프트 설정\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 {subject}에 능숙한 비서입니다\"), #시스템메세지\n",
    "    MessagesPlaceholder(variable_name=\"history\"),   #플레이스홀더\n",
    "    (\"human\", \"{question}\")                         # 사용자메세지\n",
    "])\n",
    "\n",
    "chain = prompt | ChatOpenAI(model='gpt-4.1-mini')   #프롬프트와 모델 연결\n",
    "\n",
    "# 히스토리 관리 추가\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='1+2는 3입니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 31, 'total_tokens': 39, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_38647f5e19', 'id': 'chatcmpl-BbJxCuckA3L1msXo2lCTunfYJlGis', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--9932706f-77c3-4b41-8da0-f21dd569ac09-0' usage_metadata={'input_tokens': 31, 'output_tokens': 8, 'total_tokens': 39, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# 체인 실행\n",
    "response = chain_with_history.invoke(\n",
    "    {\"subject\": \"수학\", \"question\": \"1+2는 얼마인가요?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user1\"}}\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='1+2는 얼마인가요?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='1+2는 3입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 31, 'total_tokens': 39, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_38647f5e19', 'id': 'chatcmpl-BbJxCuckA3L1msXo2lCTunfYJlGis', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--9932706f-77c3-4b41-8da0-f21dd569ac09-0', usage_metadata={'input_tokens': 31, 'output_tokens': 8, 'total_tokens': 39, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 세션 ID로 히스토리 가져오기\n",
    "get_session_history(\"user1\").messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='기존 값인 3에 숫자 2를 곱하면 3 × 2 = 6입니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 61, 'total_tokens': 86, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_79b79be41f', 'id': 'chatcmpl-BbJzoPfYcFMy35bpHKv4r3MCdyIno', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--4f78ff62-d932-4cea-9a64-f4e2a3dc0b03-0' usage_metadata={'input_tokens': 61, 'output_tokens': 25, 'total_tokens': 86, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# 히스토리 이용해서 대화 진행\n",
    "response = chain_with_history.invoke(\n",
    "    {\"subject\": \"수학\", \"question\": \"여기에 숫자 2를 곱하면 얼마인가요?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user1\"}}\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='1+2는 얼마인가요?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='1+2는 3입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 31, 'total_tokens': 39, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_38647f5e19', 'id': 'chatcmpl-BbJxCuckA3L1msXo2lCTunfYJlGis', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--9932706f-77c3-4b41-8da0-f21dd569ac09-0', usage_metadata={'input_tokens': 31, 'output_tokens': 8, 'total_tokens': 39, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='여기에 숫자 2를 곱하면 얼마인가요?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='기존 값인 3에 숫자 2를 곱하면 3 × 2 = 6입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 61, 'total_tokens': 86, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_79b79be41f', 'id': 'chatcmpl-BbJzoPfYcFMy35bpHKv4r3MCdyIno', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4f78ff62-d932-4cea-9a64-f4e2a3dc0b03-0', usage_metadata={'input_tokens': 61, 'output_tokens': 25, 'total_tokens': 86, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 세션 ID로 히스토리 가져오기\n",
    "get_session_history(\"user1\").messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 에이전트 (Agent)\n",
    "- 자율적 의사결정이 가능한 실행 단위\n",
    "- LangChain에서는 Agent 클래스를 통해 에이전트 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `add` with `{'a': 100, 'b': 200}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m300.0\u001b[0m\u001b[32;1m\u001b[1;3m100과 200을 더하면 300입니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '100과 200을 더하면 얼마인가요?', 'output': '100과 200을 더하면 300입니다.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# 프롬프트 템플릿 생성 - ReAct 에이전트에 필요한 변수들 포함\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 친절한 수학 선생님입니다.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\") # 작업한 내용들을 기록\n",
    "])\n",
    "\n",
    "# 도구 정의\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"두 숫자를 더하는 도구\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def subtract(a: float, b: float) -> float:\n",
    "    \"\"\"두 숫자를 빼는 도구\"\"\"\n",
    "    return a - b\n",
    "\n",
    "# 도구 목록 생성\n",
    "tools = [\n",
    "    add,\n",
    "    subtract\n",
    "]\n",
    "\n",
    "# 에이전트 생성 (도구 호출)\n",
    "agent = create_tool_calling_agent(\n",
    "    llm=ChatOpenAI(model='gpt-4.1-mini'),\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# 에이전트 실행 도구 정의\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,      # 도구 호출 에이전트\n",
    "    tools=tools,      # 도구 목록\n",
    "    verbose=True,     # 상세 로그 출력\n",
    "    )\n",
    "\n",
    "# 에이전트 실행\n",
    "agent_executor.invoke({\"input\": \"100과 200을 더하면 얼마인가요?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
